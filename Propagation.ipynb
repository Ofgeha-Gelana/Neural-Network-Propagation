{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is Neural Network?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is Forward Propagation?\n",
    "\n",
    "The process where your input data races through the network, getting transformed and processed at each step until it crosses the finish line as an output.\n",
    "\n",
    "In technical terms, forward propagation involves passing the input data through the network’s layers, applying weights, biases, and activation functions to produce a final prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input Layer - the first layer of the network.\n",
    "            - receives the raw input data directly (e.g., images, text, or numerical data).\n",
    "            - The number of neurons in the input layer equals the number of features in the dataset.(eg. car age, Engine Power)\n",
    "\n",
    "Hidden Layers - These layers are where most of the computations happen.\n",
    "              - Each neuron in a hidden layer takes the weighted sum of inputs from the previous layer, adds a bias, and applies an activation function.\n",
    "              - The number of hidden layers and neurons in each layer can vary based on the network architecture.\n",
    "\n",
    "Output Layer  - The final layer of the network.\n",
    "              - produces the final prediction or classification result.\n",
    "              - The number of neurons depends on the type of task:\n",
    "\n",
    "            For binary classification: 1 neuron with Sigmoid activation.\n",
    "            For multi-class classification: Multiple neurons with Softmax activation.\n",
    "            For regression tasks: 1 neuron with no activation or linear activation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keywords\n",
    "Bias : A constant value added to the weighted sum of inputs in each neuron. It helps the network shift the activation function curve to better fit the data. it's added to the output of each neuron after the weighted sum of inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is backward propagation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backward propagation is the process where the model learns from its mistakes after making predictions. After forward propagation, where the network makes a prediction, backward propagation updates the weights and biases of the network to reduce errors, so the model can perform better in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In technical terms, backward propagation involves calculating the gradient of the loss function with respect to each weight and bias by applying the chain rule of calculus. This gradient tells you how to adjust the weights and biases to minimize the error. Here’s how it works:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Loss Calculation\n",
    "After forward propagation, the network produces an output. The difference between this output and the true label (ground truth) is the error.\n",
    "The loss function calculates the magnitude of this error. For example:\n",
    "For regression, Mean Squared Error (MSE) is often used.\n",
    "For classification, Cross-Entropy Loss might be used.\n",
    "The loss quantifies how far the prediction is from the actual value.\n",
    "2. Gradient of the Loss\n",
    "The goal of backward propagation is to calculate how the loss changes as a function of each weight and bias.\n",
    "The gradient of the loss function is computed with respect to each weight and bias. This tells you the direction and magnitude by which you should adjust the weights and biases.\n",
    "Gradients are computed using the chain rule of calculus, which essentially breaks down the derivative into smaller pieces, making it possible to update weights and biases from the output layer back to the input layer.\n",
    "3. Backpropagation Through Layers\n",
    "Output Layer: Start at the output layer and calculate how much the loss changes in response to the output neuron. This involves computing the derivative of the loss function with respect to the activations of the output neuron.\n",
    "If the task is binary classification, for example, the gradient is calculated by finding the derivative of the Sigmoid activation function. For multi-class classification, the gradient is calculated with the Softmax activation function and Cross-Entropy Loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hidden Layers: Once the gradient of the loss with respect to the output layer is known, propagate this error backward through each hidden layer.\n",
    "For each hidden neuron, you calculate the gradient of the loss with respect to the neuron’s weights and biases.\n",
    "This is done by calculating the derivative of the activation function at each hidden layer (e.g., ReLU, Sigmoid, or Tanh).\n",
    "Input Layer: Continue backpropagating the gradient to the input layer. While you typically don’t update weights at the input layer, the gradients inform how the earlier layers should be adjusted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
